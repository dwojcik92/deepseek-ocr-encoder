{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "518353ef-57c2-432c-b729-82468dc266ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "684b8e2b-def8-48b8-97bf-ee937c8a0752",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'deepseek-ai/DeepSeek-OCR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fee9676-abab-4448-987d-105f7b4f4e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of DeepseekOCRForCausalLM were not initialized from the model checkpoint at deepseek-ai/DeepSeek-OCR and are newly initialized: ['model.vision_model.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, \n",
    "                                  _attn_implementation='eager', \n",
    "                                  trust_remote_code=True, \n",
    "                                  use_safetensors=True)\n",
    "model = model.eval().cuda().to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86ec2041-3548-455c-ae9b-44a10a506b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"<image>\\nFree OCR. \"\n",
    "prompt = \"<image>\\n<|grounding|>Convert the document to markdown. \"\n",
    "image_file = 'example1.png'\n",
    "output_path = 'output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2200a9be-c6ba-4524-9d93-e27168568c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer(self, tokenizer, prompt='', image_file='', output_path = ' ', base_size = 1024, image_size = 640, crop_mode = True, test_compress = False, save_results = False):\n",
    "\n",
    "# Tiny: base_size = 512, image_size = 512, crop_mode = False\n",
    "# Small: base_size = 640, image_size = 640, crop_mode = False\n",
    "# Base: base_size = 1024, image_size = 1024, crop_mode = False\n",
    "# Large: base_size = 1280, image_size = 1280, crop_mode = False\n",
    "\n",
    "# Gundam: base_size = 1024, image_size = 640, crop_mode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f3471462-637f-4c11-904b-6a3b51a3073d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "BASE:  torch.Size([1, 256, 1280])\n",
      "PATCHES:  torch.Size([4, 100, 1280])\n",
      "=====================\n",
      "<|ref|>text<|/ref|><|det|>[[58, 145, 390, 165]]<|/det|>\n",
      "Dodanie produktu do koszyka nie oznacza jego rezerwacji \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[37, 210, 214, 230]]<|/det|>\n",
      "Liczba produktów w koszyku: 1 \n",
      "\n",
      "<|ref|>image<|/ref|><|det|>[[88, 300, 179, 430]]<|/det|>\n",
      " \n",
      "\n",
      "<|ref|>sub_title<|/ref|><|det|>[[244, 255, 366, 280]]<|/det|>\n",
      "Galaxy S25 Edge \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[244, 297, 360, 315]]<|/det|>\n",
      "Tytanowy Srebrny, 512 GB \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[244, 322, 325, 338]]<|/det|>\n",
      "SM-S937BZSGEUE \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[244, 346, 315, 362]]<|/det|>\n",
      "Przedsprzedaż \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[244, 369, 417, 386]]<|/det|>\n",
      "Wysyłka od: 26 maj 2025 do 13 cze 2025 \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[520, 255, 600, 280]]<|/det|>\n",
      "4 399,00 zł \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[480, 280, 600, 310]]<|/det|>\n",
      "Najniższa cena z 30 dni\n",
      "przed obniżką: 5 899,00 zł \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[500, 308, 600, 323]]<|/det|>\n",
      "Rabat 1 500,00 zł \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[520, 343, 600, 368]]<|/det|>\n",
      " \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[480, 410, 600, 430]]<|/det|>\n",
      "- 1 + \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[660, 145, 744, 165]]<|/det|>\n",
      "Kod promocyjny \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[660, 170, 724, 190]]<|/det|>\n",
      "Wpisz kod \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[660, 202, 815, 220]]<|/det|>\n",
      "Kod promocyjny został zastosowany. \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[865, 240, 950, 258]]<|/det|>\n",
      "- 1 000,00 zł \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[660, 286, 870, 306]]<|/det|>\n",
      "Sprawdź dostępne kody promocyjne \n",
      "\n",
      "<|ref|>sub_title<|/ref|><|det|>[[660, 353, 787, 377]]<|/det|>\n",
      "Podsumowanie \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[660, 393, 950, 411]]<|/det|>\n",
      "Cena 5 899,00 zł \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[660, 417, 950, 435]]<|/det|>\n",
      "Łączne oszczędności 1 500,00 zł \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[660, 441, 950, 459]]<|/det|>\n",
      "W tym VAT 822,58 zł \n",
      "\n",
      "<|ref|>image<|/ref|><|det|>[[48, 512, 84, 543]]<|/det|>\n",
      " \n",
      "\n",
      "<|ref|>sub_title<|/ref|><|det|>[[98, 512, 204, 533]]<|/det|>\n",
      "Program Odkup \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[98, 538, 377, 557]]<|/det|>\n",
      "Odsprzedaj swoje stare urządzenie w Programie Odkup! \n",
      "\n",
      "<|ref|>image<|/ref|><|det|>[[48, 601, 84, 632]]<|/det|>\n",
      " \n",
      "\n",
      "<|ref|>sub_title<|/ref|><|det|>[[98, 605, 220, 625]]<|/det|>\n",
      "Samsung Care + \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[98, 631, 570, 666]]<|/det|>\n",
      "Nie martw się więcej kosztami naprawy, zalaniem telefonu lub kradzieżą. Dodaj ubezpieczenie Samsung Care+. \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[520, 512, 590, 533]]<|/det|>\n",
      "+ Dodaj \n",
      "\n",
      "<|ref|>sub_title<|/ref|><|det|>[[660, 508, 950, 537]]<|/det|>\n",
      "Suma 4 399,00 zł \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[660, 562, 891, 580]]<|/det|>\n",
      "Aktualny poziom Samsung Rewards: Platinum \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[660, 595, 840, 614]]<|/det|>\n",
      "Zyskaj punkty za to zamówienie \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[660, 614, 860, 631]]<|/det|>\n",
      "(wyłącznie dla członków Samsung Rewards) \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[760, 655, 844, 673]]<|/det|>\n",
      "Dane i dostawa \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[95, 700, 456, 720]]<|/det|>\n",
      "Wysyłka odbywać się będzie od 26.05.2025 do 13.06.2025 \n",
      "\n",
      "<|ref|>image<|/ref|><|det|>[[48, 777, 84, 808]]<|/det|>\n",
      " \n",
      "\n",
      "<|ref|>sub_title<|/ref|><|det|>[[98, 780, 357, 801]]<|/det|>\n",
      "Program Odkup - nawet 2000 zł zwrotu \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[98, 808, 510, 826]]<|/det|>\n",
      "Wyćeń swoje obecne urządzenie i zyskaj nawet 2000 zł zwrotu w Programie Odkup \n",
      "\n",
      "<|ref|>image<|/ref|><|det|>[[48, 872, 84, 903]]<|/det|>\n",
      " \n",
      "\n",
      "<|ref|>sub_title<|/ref|><|det|>[[98, 874, 279, 895]]<|/det|>\n",
      "Do 800 zł zniżki w zestawie \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[98, 901, 576, 936]]<|/det|>\n",
      "Kup smartfon w zestawie i zyskaj zniżkę - 100 zł zniżki na wybrany smartwatch i słuchawki, 300 zł zniżki na wybrany tablet oraz Galaxy Ring.\n",
      "==================================================\n",
      "image size:  (1247, 1041)\n",
      "valid image tokens:  613\n",
      "output texts tokens (valid):  1217\n",
      "compression ratio:  1.99\n",
      "==================================================\n",
      "===============save results:===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image: 100%|██████████| 5/5 [00:00<00:00, 38199.49it/s]\n",
      "other: 100%|██████████| 36/36 [00:00<00:00, 137393.03it/s]\n"
     ]
    }
   ],
   "source": [
    "res = model.infer(tokenizer, prompt=prompt, image_file=image_file, output_path = output_path, base_size = 1024, image_size = 640, crop_mode=True, save_results = True, test_compress = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae258752-b17f-450f-9e10-d14d31585865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepseekOCRModel(\n",
       "  (embed_tokens): Embedding(129280, 1280)\n",
       "  (layers): ModuleList(\n",
       "    (0): DeepseekV2DecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (o_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): DeepseekV2MLP(\n",
       "        (gate_proj): Linear(in_features=1280, out_features=6848, bias=False)\n",
       "        (up_proj): Linear(in_features=1280, out_features=6848, bias=False)\n",
       "        (down_proj): Linear(in_features=6848, out_features=1280, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): DeepseekV2RMSNorm()\n",
       "      (post_attention_layernorm): DeepseekV2RMSNorm()\n",
       "    )\n",
       "    (1-11): 11 x DeepseekV2DecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (v_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (o_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): DeepseekV2MoE(\n",
       "        (experts): ModuleList(\n",
       "          (0-63): 64 x DeepseekV2MLP(\n",
       "            (gate_proj): Linear(in_features=1280, out_features=896, bias=False)\n",
       "            (up_proj): Linear(in_features=1280, out_features=896, bias=False)\n",
       "            (down_proj): Linear(in_features=896, out_features=1280, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (gate): MoEGate()\n",
       "        (shared_experts): DeepseekV2MLP(\n",
       "          (gate_proj): Linear(in_features=1280, out_features=1792, bias=False)\n",
       "          (up_proj): Linear(in_features=1280, out_features=1792, bias=False)\n",
       "          (down_proj): Linear(in_features=1792, out_features=1280, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (input_layernorm): DeepseekV2RMSNorm()\n",
       "      (post_attention_layernorm): DeepseekV2RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): DeepseekV2RMSNorm()\n",
       "  (sam_model): ImageEncoderViT(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (neck): Sequential(\n",
       "      (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): LayerNorm2d()\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (3): LayerNorm2d()\n",
       "    )\n",
       "    (net_2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (net_3): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  )\n",
       "  (vision_model): VitModel(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "      (position_embedding): Embedding(257, 1024)\n",
       "    )\n",
       "    (transformer): NoTPTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x NoTPTransformerBlock(\n",
       "          (self_attn): NoTPAttention(\n",
       "            (qkv_proj): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (mlp): NoTPFeedForward(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (projector): MlpProjector(\n",
       "    (layers): Linear(in_features=2048, out_features=1280, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "11e69a22-0ac6-4553-994f-363c9b563258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a4c3bf6-9024-4f9e-8072-f556f57a3db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def deepencoder_tokens_via_clip(model, png_path, device=None):\n",
    "    \"\"\"\n",
    "    Implements: SAM-base -> 2-layer conv compressor (already inside sam_model.neck/net_2/net_3)\n",
    "                 -> flatten to tokens -> add CLIP pos-emb (no CLS)\n",
    "                 -> CLIP pre-layernorm + transformer\n",
    "    Returns: tokens_clip  [B, N, 1024]\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "\n",
    "    img = Image.open(png_path).convert(\"RGB\")\n",
    "    x = _preproc_1024(img).unsqueeze(0).to(torch.bfloat16).to(device)  # [1,3,1024,1024] -> N=256\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    # --- SAM-base encoder + 16× token compressor ---\n",
    "    # Your ImageEncoderViT already contains: patch_embed (stride 16) + blocks + neck + net_2 (stride 2) + net_3 (stride 2).\n",
    "    # Its output tensor should be [B, 1024, Hs, Ws] with Hs*Ws = N.\n",
    "    sam_out = model.base_model.sam_model(x)\n",
    "    B, C, Hs, Ws = sam_out.shape\n",
    "    assert C == 1024, f\"Expect 1024 channels before CLIP, got {C}\"\n",
    "\n",
    "    # Flatten to tokens [B, N, 1024], N = Hs*Ws = (H/16/4)*(W/16/4) = HW/4096\n",
    "    tokens = sam_out.flatten(2).transpose(1, 2).contiguous()  # [B,N,1024]\n",
    "    N = tokens.size(1)\n",
    "\n",
    "    # --- Add CLIP positional embeddings (no CLS token) ---\n",
    "    # CLIP-large position table in your model: Embedding(257, 1024). Index 0 is CLS; 1..256 are 16×16 patches.\n",
    "    pos_table = model.base_model.vision_model.embeddings.position_embedding.weight  # [257,1024]\n",
    "    # Build a 2D 16×16 grid from indices 1..256, then interpolate to (Hs, Ws) if needed.\n",
    "    base_grid = pos_table[1:257].view(16, 16, 1024).permute(2, 0, 1).unsqueeze(0)  # [1,1024,16,16]\n",
    "\n",
    "    # below is optional as Hs and Ws should in 16 and 16\n",
    "    # grid = F.interpolate(base_grid, size=(Hs, Ws), mode=\"bicubic\", align_corners=False)  # [1,1024,Hs,Ws]\n",
    "    \n",
    "    pos = base_grid.flatten(2).transpose(1, 2)  # [1,N,1024] # base grid or grid if interpoaltion is needed\n",
    "    tokens = tokens + pos # or pos\n",
    "\n",
    "    # --- Run CLIP without its patch embed ---\n",
    "    x_tok = model.base_model.vision_model.pre_layrnorm(tokens)          # [B,N,1024]\n",
    "    x_tok = model.base_model.vision_model.transformer(x_tok)            # [B,N,1024] (NoTPTransformer)\n",
    "    # if isinstance(x_tok, (list, tuple)):  # some impls return (x, ...)\n",
    "    #     x_tok = x_tok[0]\n",
    "    tokens_clip = x_tok                                                   # [B,N,1024]\n",
    "\n",
    "    # Last step add sam out and CLIP out\n",
    "    out = tokens + tokens_clip\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f7fb9f6-a9b7-4711-949c-689757d1b9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import math\n",
    "from typing import Optional, Union\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "# --- Runtime knobs (safe speedups on Ampere+ with BF16) ---\n",
    "torch.backends.cuda.matmul.allow_tf32 = True         # allow TF32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True               # allow TF32 on cudnn\n",
    "torch.backends.cudnn.benchmark = True                # autotune convs\n",
    "# Prefer Flash SDP if your PyTorch build supports it (falls back automatically)\n",
    "try:\n",
    "    torch.backends.cuda.enable_flash_sdp(True)\n",
    "    torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "    torch.backends.cuda.enable_math_sdp(False)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "\n",
    "class DeepSeekOCREncoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Optimized, memory-lean encoder:\n",
    "      SAM-base (with built-in conv compressor) -> [B,1024,Hs,Ws]\n",
    "      flatten -> add CLIP 2D pos-emb (no CLS)\n",
    "      -> CLIP pre-layernorm + transformer\n",
    "      returns tokens + CLIP(tokens)  [B, N, 1024]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        full_model,\n",
    "        device: Optional[Union[str, torch.device]] = None,\n",
    "        dtype: torch.dtype = torch.bfloat16,\n",
    "        freeze: bool = True,\n",
    "        eager_to_device: bool = True,\n",
    "        precompute_pos_for_1024: bool = True,\n",
    "        use_compile: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if device is None:\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.device = torch.device(device)\n",
    "        self.dtype = dtype\n",
    "        self.embed_dim = 1024\n",
    "\n",
    "        base = getattr(full_model, \"base_model\", full_model)\n",
    "        self.sam = getattr(base, \"sam_model\").eval()\n",
    "        vision = getattr(base, \"vision_model\")\n",
    "        self.clip_pre = vision.pre_layrnorm.eval()\n",
    "        self.clip_tr = vision.transformer.eval()\n",
    "\n",
    "        # Position table (register as buffer so it moves with .to())\n",
    "        pos_table = vision.embeddings.position_embedding.weight.detach()  # [1+G^2, 1024]\n",
    "        self.register_buffer(\"clip_pos_table\", pos_table, persistent=False)\n",
    "\n",
    "        # Drop likely-unused heavy parts to save RAM/VRAM\n",
    "        def _safe_del(obj, name):\n",
    "            if hasattr(obj, name):\n",
    "                try:\n",
    "                    setattr(obj, name, None)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        _safe_del(vision.embeddings, \"patch_embedding\")\n",
    "        _safe_del(vision, \"post_layernorm\")\n",
    "        _safe_del(base, \"text_model\")\n",
    "        _safe_del(base, \"lm_head\")\n",
    "        _safe_del(full_model, \"text_model\")\n",
    "        _safe_del(full_model, \"lm_head\")\n",
    "\n",
    "        # If you want N=256 like the paper figure, feed 1024×1024.\n",
    "        self._preproc_1024 = transforms.Compose([\n",
    "            transforms.Resize((1024, 1024), interpolation=transforms.InterpolationMode.BICUBIC, antialias=True),\n",
    "            transforms.ToTensor(),\n",
    "            # Use the SAM/CLIP-normalization your training used. If unsure, CLIP's works well in practice:\n",
    "            transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073),\n",
    "                                 std=(0.26862954, 0.26130258, 0.27577711)),\n",
    "        ])\n",
    "\n",
    "        if freeze:\n",
    "            for m in [self.sam, self.clip_pre, self.clip_tr]:\n",
    "                for p in m.parameters():\n",
    "                    p.requires_grad_(False)\n",
    "\n",
    "        if eager_to_device:\n",
    "            # channels_last helps conv-heavy encoders; safe for BF16\n",
    "            self.sam.to(self.device, dtype=self.dtype, memory_format=torch.channels_last)\n",
    "            self.clip_pre.to(self.device, dtype=self.dtype)\n",
    "            self.clip_tr.to(self.device, dtype=self.dtype)\n",
    "            self.clip_pos_table.data = self.clip_pos_table.to(self.device, dtype=self.dtype)\n",
    "\n",
    "        # Precompute pos-emb for the common 1024→N=256 case (Hs=Ws=16)\n",
    "        self._pos_cache = {}\n",
    "        self._pos_fixed_16 = None\n",
    "        if precompute_pos_for_1024:\n",
    "            table = self.clip_pos_table\n",
    "            grid_size = int(math.isqrt(table.size(0) - 1))\n",
    "            if grid_size * grid_size + 1 != table.size(0):\n",
    "                raise RuntimeError(\"Position table size is not 1+G^2; cannot infer grid size.\")\n",
    "            base_grid = table[1: 1 + grid_size * grid_size]  # [G^2, 1024]\n",
    "            base_grid = base_grid.view(grid_size, grid_size, self.embed_dim).permute(2, 0, 1).unsqueeze(0)  # [1,1024,G,G]\n",
    "            if grid_size != 16:\n",
    "                base_grid = F.interpolate(base_grid, size=(16, 16), mode=\"bicubic\", align_corners=False)\n",
    "            self._pos_fixed_16 = base_grid.flatten(2).transpose(1, 2).contiguous()  # [1,256,1024]\n",
    "\n",
    "        # Optional torch.compile for better kernel fusion on the transformer path\n",
    "        self._compiled = None\n",
    "        if use_compile and hasattr(torch, \"compile\"):\n",
    "            self._compiled = torch.compile(self._forward_core, mode=\"max-autotune\")  # PyTorch 2.3+\n",
    "\n",
    "        # Optional CUDA Graphs (opt-in; call capture_cudagraph() once before steady-state)\n",
    "        self._graph = None\n",
    "        self._static_in = None\n",
    "        self._static_out = None\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def _forward_core(self, x_nchw_channels_last: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x_nchw_channels_last: [B,3,1024,1024] BF16 on self.device, channels_last\n",
    "        \"\"\"\n",
    "        sam_out = self.sam(x_nchw_channels_last)  # [B, 1024, Hs, Ws]\n",
    "        B, C, Hs, Ws = sam_out.shape\n",
    "        if C != self.embed_dim:\n",
    "            raise RuntimeError(f\"Expected {self.embed_dim} channels before CLIP, got {C}\")\n",
    "\n",
    "        tokens = sam_out.flatten(2).transpose(1, 2).contiguous()  # [B, N, 1024], N=Hs*Ws\n",
    "\n",
    "        if Hs == 16 and Ws == 16 and self._pos_fixed_16 is not None:\n",
    "            pos = self._pos_fixed_16  # [1,256,1024]\n",
    "        else:\n",
    "            key = (Hs, Ws)\n",
    "            pos = self._pos_cache.get(key)\n",
    "            if pos is None:\n",
    "                table = self.clip_pos_table\n",
    "                grid_size = int(math.isqrt(table.size(0) - 1))\n",
    "                base_grid = table[1: 1 + grid_size * grid_size].view(grid_size, grid_size, self.embed_dim)\n",
    "                base_grid = base_grid.permute(2, 0, 1).unsqueeze(0)  # [1,1024,G,G]\n",
    "                if (Hs, Ws) != (grid_size, grid_size):\n",
    "                    base_grid = F.interpolate(base_grid, size=(Hs, Ws), mode=\"bicubic\", align_corners=False)\n",
    "                pos = base_grid.flatten(2).transpose(1, 2).contiguous()  # [1,N,1024]\n",
    "                self._pos_cache[key] = pos\n",
    "\n",
    "        tokens_plus = tokens + pos  # broadcast over batch\n",
    "        x_tok = self.clip_pre(tokens_plus)\n",
    "        x_tok = self.clip_tr(x_tok)\n",
    "        return tokens + x_tok  # [B,N,1024]\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def encode(self, image: Union[Image.Image, str, \"os.PathLike\"]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        image can be a PIL.Image or a path-like to an RGB image.\n",
    "        Returns: [1, N, 1024]  (N=256 for 1024×1024 input)\n",
    "        \"\"\"\n",
    "        if isinstance(image, (str, bytes)):\n",
    "            img = Image.open(image).convert(\"RGB\")\n",
    "        else:\n",
    "            img = image.convert(\"RGB\")\n",
    "\n",
    "        # CPU preproc → pinned → non_blocking H2D\n",
    "        x_cpu = self._preproc_1024(img).unsqueeze(0).pin_memory()                 # [1,3,1024,1024] on CPU\n",
    "        x = x_cpu.to(self.device, dtype=self.dtype, non_blocking=True)       # -> GPU BF16\n",
    "        x = x.to(memory_format=torch.channels_last)                          # NHWC memory layout\n",
    "\n",
    "        # Fast path: CUDA Graph replay if captured\n",
    "        if self._graph is not None:\n",
    "            self._static_in.copy_(x)                                         # copy into static buffer\n",
    "            self._graph.replay()\n",
    "            return self._static_out\n",
    "\n",
    "        # Compiled path if enabled; otherwise plain\n",
    "        if self._compiled is not None:\n",
    "            return self._compiled(x)\n",
    "        else:\n",
    "            return self._forward_core(x)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def capture_cudagraph(self, batch_size: int = 1, H: int = 1024, W: int = 1024):\n",
    "        \"\"\"\n",
    "        Capture a CUDA graph for the steady-state 1×3×1024×1024 path.\n",
    "        Call once after initialization; then .encode(...) will reuse it.\n",
    "        \"\"\"\n",
    "        if self.device.type != \"cuda\":\n",
    "            raise RuntimeError(\"CUDA Graphs require CUDA device.\")\n",
    "        if self._graph is not None:\n",
    "            return  # already captured\n",
    "\n",
    "        # Static input/output buffers (must not resize later)\n",
    "        static_in = torch.empty(\n",
    "            batch_size, 3, H, W, device=self.device, dtype=self.dtype\n",
    "        ).to(memory_format=torch.channels_last)\n",
    "        # Warm-up to materialize kernels & memory pools\n",
    "        for _ in range(3):\n",
    "            _ = self._forward_core(static_in)\n",
    "\n",
    "        g = torch.cuda.CUDAGraph()\n",
    "        torch.cuda.synchronize()\n",
    "        with torch.cuda.graph(g):\n",
    "            static_out = self._forward_core(static_in)\n",
    "        self._graph = g\n",
    "        self._static_in = static_in\n",
    "        self._static_out = static_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fa70430-3dfe-433b-ac9e-685369455dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31c02e32-b6a7-46da-bddf-1b7a21666b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50facba0-4d80-4d2d-82ca-1c0e9e42f3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of DeepseekOCRForCausalLM were not initialized from the model checkpoint at deepseek-ai/DeepSeek-OCR and are newly initialized: ['model.vision_model.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"deepseek-ai/DeepSeek-OCR\"\n",
    "\n",
    "# If your env supports it, prefer SDPA/Flash attention over eager.\n",
    "# In recent transformers the arg is `attn_implementation`; your underscore version also works for some models.\n",
    "attn_kw = dict(attn_implementation=\"eager\")  # or \"flash_attention_2\" if installed; else keep \"eager\"\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    use_safetensors=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    **attn_kw,\n",
    ")\n",
    "\n",
    "model = model.eval().to(\"cuda\", dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a80b4ed-a10a-4852-be4e-e7531a5b8155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the lean encoder\n",
    "enc = DeepSeekOCREncoder(\n",
    "    full_model=model,\n",
    "    device=\"cuda\",\n",
    "    dtype=torch.bfloat16,\n",
    "    freeze=True,\n",
    "    eager_to_device=True,\n",
    "    precompute_pos_for_1024=True,\n",
    "    use_compile=False,  # set True if you're on PyTorch 2.3+ and want extra fusion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "366a535a-1d1a-4e46-9cae-4641493a0835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: capture a CUDA Graph for 1×3×1024×1024 steady-state inference\n",
    "enc.capture_cudagraph(batch_size=1, H=1024, W=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7790b257-8a32-41a4-b737-0aaeab8cad12",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file = 'example1.png'\n",
    "img = Image.open(image_file).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ccfe4b7-bfd6-4274-96f3-d18323e1f4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 4.08 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "58.7 ms ± 40.5 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit vision_tokens = enc.encode(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b71826d-73d9-473d-995c-7e6dac1e3983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 1024])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vision_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39576070-13be-4dd9-ad7c-4e3ffcb41b14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
